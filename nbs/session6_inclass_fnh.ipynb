{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Benchmark classification on ```cifar-10```\n",
    "\n",
    "This notebook builds on what we were doing last week with the handwritten digits from the MNIST dataset.\n",
    "\n",
    "This week, we're working with another famous dataset in computer vision and image processing research - [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path tools\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "# data loader\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classificatio models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7343d4b5",
   "metadata": {},
   "source": [
    "We're going to load the data using a function from the library ```TensorFlow```, which we'll be looking at in more detail next week. \n",
    "\n",
    "For now, we're just using it to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # cifar10 method \"load_data\" its returning 4 object. training data and label + test data and label\n",
    "                                                           # gives touple "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b588be73",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # shape = 4 numbers = 4 dimensions\n",
    "              # 50000 = amount of images \n",
    "              # 32 x 32 pixel intensity\n",
    "              # 3 = the 3 colour channels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd02fcbe",
   "metadata": {},
   "source": [
    "Unfortunately, this version of the data set doesn't have explict labels, so we need to create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \n",
    "          \"automobile\", \n",
    "          \"bird\", \n",
    "          \"cat\", \n",
    "          \"deer\", \n",
    "          \"dog\", \n",
    "          \"frog\", \n",
    "          \"horse\", \n",
    "          \"ship\", \n",
    "          \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all the data to greyscale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5391f3",
   "metadata": {},
   "source": [
    "In the following cell, I'm converting all of my images to greyscale and then making a ```numpy``` array at the end.\n",
    "\n",
    "Notice that I'm using something funky here called *[list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train]) #list comprehensions = \n",
    "X_test_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8934cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension\n",
    "\n",
    "# for loop example\n",
    "\n",
    "# for x in y:\n",
    "    # do_this(x)\n",
    "\n",
    "#list comprehension example\n",
    "\n",
    "#[do_this(x) for x in y]\n",
    "\n",
    "#example with a list of colours\n",
    "  \n",
    "#colours = [\"red\", \"gren\", \"blue\"]\n",
    "#uppers = []\n",
    "\n",
    "#for colour in colours:\n",
    "    #upper = colour.upper()\n",
    "    #uppers.append(upper)\n",
    "\n",
    "\n",
    "#uppers = [colour.upper() for colour in colours]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9703dbdc",
   "metadata": {},
   "source": [
    "Then, we're going to do some simple scaling by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train_grey)/255.0 # pixel values scaled down between 0 and 1\n",
    "X_test_scaled = (X_test_grey)/255.0 # just compressing the pixel values. You can recreate it by multiplying it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c141a5e2",
   "metadata": {},
   "source": [
    "Next, we're going to reshape this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n samples is 50000, nx = number of values on y and y 32. then we reshape the x_train_scaled into a 1-dimensional array. \n",
    "# by telling numpy that it shall time nx by ny in order to flatten\n",
    "nsamples, nx, ny = X_train_scaled.shape \n",
    "X_train_dataset = X_train_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_test_scaled.shape\n",
    "X_test_dataset = X_test_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e24b419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1024)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking that the training data has been flattened to 1 dimension\n",
    "X_train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127f9d5",
   "metadata": {},
   "source": [
    "logistic regression classifier is a good initial approach to scratch the surface. But for more in depth machine learning and prediction use Neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bdea84",
   "metadata": {},
   "source": [
    "We define our Logistic Regression classifier as we have done previously. You'll notice that I've set a lot of different parameters here - you can learn more in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.22068667\n",
      "Epoch 3, change: 0.15329279\n",
      "Epoch 4, change: 0.10016197\n",
      "convergence after 5 epochs took 13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.6s finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"none\", # force our model to make very small values be set to zero. tool for only keeping the most meaningful weights in a model. if set to L1 or L2 penalties\n",
    "                        tol=0.1, # tolerance = this is by how much weights should be changig when our model improves every time. if it doesnt improve to satisfy the tolerance, the model stops. If the weights arent modified by more than 0.1 \n",
    "                        verbose=True, # if verbose set to true it gives a rolling update of how the model is performing\n",
    "                        solver=\"saga\", \n",
    "                        multi_class=\"multinomial\").fit(X_train_dataset, y_train) # its a multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc10cdb4",
   "metadata": {},
   "source": [
    "We can then print our classification report, using the label names that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.35      0.36      0.35      1000\n",
      "  automobile       0.37      0.40      0.38      1000\n",
      "        bird       0.25      0.26      0.25      1000\n",
      "         cat       0.23      0.14      0.17      1000\n",
      "        deer       0.26      0.16      0.19      1000\n",
      "         dog       0.27      0.35      0.31      1000\n",
      "        frog       0.29      0.30      0.30      1000\n",
      "       horse       0.31      0.32      0.32      1000\n",
      "        ship       0.35      0.40      0.37      1000\n",
      "       truck       0.39      0.44      0.41      1000\n",
      "\n",
      "    accuracy                           0.31     10000\n",
      "   macro avg       0.31      0.31      0.31     10000\n",
      "weighted avg       0.31      0.31      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels) # using labels assigned earlier \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f6d9b4",
   "metadata": {},
   "source": [
    "I've set a couple of different parameters here - you can see more in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "**NB!** This will take a long time to run! On the 32 CPU machine on UCloud, this takes around 30 seconds per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.18081081\n",
      "Validation score: 0.230200\n",
      "Iteration 2, loss = 2.03512719\n",
      "Validation score: 0.285000\n",
      "Iteration 3, loss = 1.96959989\n",
      "Validation score: 0.277400\n",
      "Iteration 4, loss = 1.93144341\n",
      "Validation score: 0.313000\n",
      "Iteration 5, loss = 1.89991063\n",
      "Validation score: 0.318000\n",
      "Iteration 6, loss = 1.86922342\n",
      "Validation score: 0.334000\n",
      "Iteration 7, loss = 1.84902258\n",
      "Validation score: 0.340000\n",
      "Iteration 8, loss = 1.82470768\n",
      "Validation score: 0.340600\n",
      "Iteration 9, loss = 1.80246310\n",
      "Validation score: 0.347600\n",
      "Iteration 10, loss = 1.78740588\n",
      "Validation score: 0.353000\n",
      "Iteration 11, loss = 1.77777913\n",
      "Validation score: 0.353200\n",
      "Iteration 12, loss = 1.75619007\n",
      "Validation score: 0.360800\n",
      "Iteration 13, loss = 1.74167545\n",
      "Validation score: 0.368000\n",
      "Iteration 14, loss = 1.73132602\n",
      "Validation score: 0.373400\n",
      "Iteration 15, loss = 1.71718153\n",
      "Validation score: 0.372400\n",
      "Iteration 16, loss = 1.70771119\n",
      "Validation score: 0.374600\n",
      "Iteration 17, loss = 1.69674908\n",
      "Validation score: 0.381800\n",
      "Iteration 18, loss = 1.68782656\n",
      "Validation score: 0.377800\n",
      "Iteration 19, loss = 1.67950203\n",
      "Validation score: 0.388200\n",
      "Iteration 20, loss = 1.66514975\n",
      "Validation score: 0.388200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42,\n",
    "                    hidden_layer_sizes=(100, 10),\n",
    "                    learning_rate=\"adaptive\", #when its gotten a rough idea of where the weights should be. Start learning, but when you achieve something, slow down and finetune. Learn at different speeds\n",
    "                    early_stopping=True, \n",
    "                    verbose=True,\n",
    "                    max_iter=20).fit(X_train_dataset, y_train) # only running for a maximum of 20 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation should be going up in percentage == loss score should be going down pr. iteration\n",
    "# the model is only using a small portion of the data to train\n",
    "# during training we minimize loss value and gain an accuracy score on the validation score.\n",
    "# this is what happens for 20 iterations\n",
    "# then we test it on data it has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e489977e",
   "metadata": {},
   "source": [
    "Lastly, we can get our classification report as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.49      0.30      0.37      1000\n",
      "  automobile       0.46      0.50      0.48      1000\n",
      "        bird       0.30      0.23      0.26      1000\n",
      "         cat       0.28      0.20      0.23      1000\n",
      "        deer       0.31      0.32      0.31      1000\n",
      "         dog       0.37      0.36      0.36      1000\n",
      "        frog       0.35      0.48      0.40      1000\n",
      "       horse       0.44      0.46      0.45      1000\n",
      "        ship       0.42      0.60      0.49      1000\n",
      "       truck       0.45      0.44      0.45      1000\n",
      "\n",
      "    accuracy                           0.39     10000\n",
      "   macro avg       0.39      0.39      0.38     10000\n",
      "weighted avg       0.39      0.39      0.38     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pipreqs\n",
    "# optimizing the packages "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5067ab",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Take the code outlined in this notebook and turn it into two separate Python scripts, one which performs Logistic Regression classification and one which uses the MLPClassifier on the ```Cifar10``` dataset.\n",
    "\n",
    "Try to use the things we've spoken about in clas\n",
    "- Requirements.txt\n",
    "- Virtual environment\n",
    "- Setup scripts\n",
    "- Argparse\n",
    "\n",
    "This task is [Assignment 2 for Visual Analytics](https://classroom.github.com/a/KLVvny7d)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
